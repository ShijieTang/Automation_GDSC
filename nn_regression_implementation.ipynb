{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d409930f-bff4-44d6-b9a7-b6eb4c6705bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "#from cuml.ensemble import RandomForestRegressor as cuRF  # not working on windows\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, DotProduct, WhiteKernel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "#from sklearn.metrics import r2_score  # Use a torch version instead.\n",
    "\n",
    "from scipy.stats import norm\n",
    "from dataloader import load_gdsc, prepare_features, split_data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736db25d-8c32-4b10-b925-b0759167edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'Name':                'Yucheng Shao',\n",
    "\n",
    "    'random_seed':         16,\n",
    "    'PCA_component':       70,        # in the inspection file, aim for 90% variance\n",
    "\n",
    "    'surrogate':           'bayes_linear',      # 'gp', 'rf', 'bayes_linear', 'mc_dropout'\n",
    "                                      # for Gaussian Process, Random Forest, and Bayesian Ridge\n",
    "    \n",
    "    'training_iter':       20,        # GP cannot run efficient enough, train for some iterations\n",
    "    'max_train_size':      1024,      # GP cannot run efficient enough, train from a subset instead\n",
    "    \n",
    "    'subsample_pool_size': 4096,      # GP cannot run efficient enough, draw from a subpool instead\n",
    "\n",
    "    'n_estimators':        100,       # RF if not efficient enough, lower this value\n",
    "    \n",
    "    'mc_droupout_T':       20,        # number of dropout models used, reduce for efficiency\n",
    "    \n",
    "    'acquisition':         'metropolis',    # 'passive', 'greedy', 'mcmc', 'thompson', 'metropolis'\n",
    "    'initial_ratio':       0.01,\n",
    "\n",
    "    'mcmc_steps':          1000,      # number of steps to average for mcmc\n",
    "    \n",
    "    'metropolis_steps':    10000,     # number of steps to do metropolis select\n",
    "\n",
    "    'val_ratio':           0.2,\n",
    "\n",
    "    'batch_size':          2048,\n",
    "    'epochs':              10,\n",
    "    'lr':                  0.0012,\n",
    "    'weight_decay':        1e-4,\n",
    "    'dropout':             0.4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e6477c-594c-4140-b42f-4346514c21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['acquisition'] = config['acquisition'].lower()\n",
    "valid_acq = {'passive', 'greedy', 'mcmc', 'thompson', 'metropolis'}\n",
    "if config['acquisition'] not in valid_acq:\n",
    "    raise ValueError(f\"Unknown acquisition type '{config['acquisition']}'. Choose from {valid_acq}\")\n",
    "\n",
    "config['surrogate'] = config['surrogate'].lower()\n",
    "valid_sur = {'gp', 'rf', 'bayes_linear', 'mc_dropout'}\n",
    "if config['surrogate'] not in valid_sur:\n",
    "    raise ValueError(f\"Unknown surrogate type '{config['surrogate']}'. Choose from {valid_sur}\")\n",
    "\n",
    "\n",
    "assert config['subsample_pool_size'] > config['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e776f3cb-ac98-45b5-b764-05c0919b929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set seed once at the beginning\n",
    "set_seed(config['random_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4985f7f4-b2fb-418a-b85b-905fe361071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_columns = ['LN_IC50', 'AUC', 'Z_SCORE', 'DRUG_ID', 'COSMIC_ID', 'DRUG_NAME', 'CELL_LINE_NAME']\n",
    "df = load_gdsc(excluded_columns=excluded_columns)   # With Drop NaN & Exclude Outlier with IQR\n",
    "\n",
    "# Create dummy variables for categorical features and split the data into training and testing sets with default test size of 0.2\n",
    "X_dummy, y = prepare_features(df, encode_dummies=True)\n",
    "X_label, _ = prepare_features(df, encode_dummies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "873acf48-152e-4de8-9d45-d03ff6480564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incase the sparse input under-perform\n",
    "X_dummy_pca = PCA(n_components=config['PCA_component']).fit_transform(X_dummy)\n",
    "# Convert back to DataFrame\n",
    "X_dummy = pd.DataFrame(X_dummy_pca, columns=[f'PC{i+1}' for i in range(X_dummy_pca.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0fdd042-2e5a-4dbd-baaa-f168888e2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd_tr, Xd_te, Xl_tr, Xl_te, y_tr, y_te = split_data(X_dummy, X_label, y) \n",
    "# Xd: X features with dummy variables\n",
    "# Xl: X features with label encoding (e.g. Turn R, G, B into 0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc053382-cf94-4548-84b0-127ad2736f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPyTorchGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPyTorchGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.mean_module(x)\n",
    "        covar = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covar)\n",
    "\n",
    "def fit_gpytorch_and_predict(X_train, y_train, X_pool, seed, training_iter=20, \n",
    "                             max_train_size=config['max_train_size']):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if len(X_train) > max_train_size:\n",
    "        np.random.seed(seed)\n",
    "        sub_idx = np.random.choice(len(X_train), max_train_size, replace=False)\n",
    "        X_train = X_train[sub_idx]\n",
    "        y_train = y_train[sub_idx]\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).view(-1).to(device)\n",
    "    X_pool = torch.tensor(X_pool, dtype=torch.float32).to(device)\n",
    "\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "    model = GPyTorchGPModel(X_train, y_train, likelihood).to(device)\n",
    "\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for _ in range(training_iter):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train)\n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        preds = model(X_pool)\n",
    "        mu = preds.mean.cpu().numpy()\n",
    "        sigma = preds.stddev.cpu().numpy()\n",
    "\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def fit_rf_sklearn_and_predict(X_train, y_train, X_pool, seed, n_estimators=config['n_estimators'], \n",
    "                               max_train_size=config['max_train_size']):\n",
    "    if len(X_train) > max_train_size:\n",
    "        np.random.seed(seed)\n",
    "        sub_idx = np.random.choice(len(X_train), max_train_size, replace=False).astype(int)\n",
    "        X_train = X_train[sub_idx]\n",
    "        y_train = y_train[sub_idx]\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train.ravel())\n",
    "\n",
    "    # Collect predictions from all trees\n",
    "    preds = np.stack([tree.predict(X_pool) for tree in rf.estimators_], axis=0)\n",
    "    \n",
    "    mu = preds.mean(axis=0)\n",
    "    sigma = preds.std(axis=0)\n",
    "\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "''' Does not work on windows!\n",
    "def fit_cuml_rf_and_predict(X_train_np, y_train_np, X_pool_np, n_estimators=100):\n",
    "    rf_model = cuRF(n_estimators=n_estimators)\n",
    "    rf_model.fit(X_train_np, y_train_np)\n",
    "\n",
    "    # predict from each tree for uncertainty estimation\n",
    "    preds = np.stack([tree.predict(X_pool_np) for tree in rf_model.base_models_], axis=0)\n",
    "    mu = preds.mean(axis=0)\n",
    "    sigma = preds.std(axis=0)\n",
    "    return mu, sigma\n",
    "'''\n",
    "\n",
    "\n",
    "def fit_bayes_linear_and_predict(X_train, y_train, X_pool, seed, \n",
    "                                 max_train_size=config['max_train_size']):\n",
    "    if len(X_train) > max_train_size:\n",
    "        np.random.seed(seed)\n",
    "        sub_idx = np.random.choice(len(X_train), max_train_size, replace=False).astype(int)\n",
    "        X_train = X_train[sub_idx]\n",
    "        y_train = y_train[sub_idx]\n",
    "    \n",
    "    model = BayesianRidge()\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    mu, sigma = model.predict(X_pool, return_std=True)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def enable_dropout(nn_model):\n",
    "    \"\"\"Enable dropout during inference.\"\"\"\n",
    "    for m in nn_model.modules():\n",
    "        if isinstance(m, nn.Dropout):\n",
    "            m.train()\n",
    "\n",
    "\n",
    "def fit_mc_dropout_and_predict(nn_model, X, T=config['mc_droupout_T']):\n",
    "    nn_model.eval()\n",
    "    enable_dropout(nn_model)\n",
    "\n",
    "    device = next(nn_model.parameters()).device\n",
    "\n",
    "    X = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(T):\n",
    "            pred = nn_model(X)\n",
    "            preds.append(pred.cpu().numpy())\n",
    "\n",
    "    preds = np.stack(preds, axis=0)  # shape: (T, batch_size, 1)\n",
    "    mu = preds.mean(axis=0).squeeze()   # mean prediction\n",
    "    sigma = preds.std(axis=0).squeeze() # uncertainty (std)\n",
    "\n",
    "    return mu, sigma\n",
    "\n",
    "#mu, sigma = fit_gpytorch_and_predict(X_train, y_train, X_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daba02e5-e9ab-448c-928b-51db245b4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "'''Define Acquisition Methods'''\n",
    "# Passive acquisition (random sampling)\n",
    "def select_next_batch_passive(X_pool, batch_size):\n",
    "    selected = np.random.choice(len(X_pool), batch_size, replace=False)\n",
    "    return selected\n",
    "    \n",
    "\n",
    "# Greedy acquisition\n",
    "def select_next_batch_greedy(X_pool, y_pool, mu, sigma, batch_size):\n",
    "    best_y = np.min(y_pool)\n",
    "    improvement = best_y - mu\n",
    "    Z = improvement / (sigma + 1e-8)\n",
    "    ei = improvement * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "    selected = np.argsort(ei)[-batch_size:]\n",
    "    return selected\n",
    "    \n",
    "\n",
    "# MCMC acquisition\n",
    "def select_next_batch_mcmc(X_pool, mu, sigma, batch_size, steps=config['mcmc_steps']):\n",
    "    scores = mu + np.random.normal(0, sigma, size=(steps, len(X_pool)))   # sample from N steps times\n",
    "    selected = np.argsort(np.mean(scores, axis=0))[-batch_size:]\n",
    "    return selected\n",
    "\n",
    "\n",
    "# Thompson Sampling\n",
    "def select_next_batch_thompson(X_pool, mu, sigma, batch_size):\n",
    "    samples = np.random.normal(mu, sigma)\n",
    "    selected = np.argsort(samples)[-batch_size:]\n",
    "    return selected\n",
    "    \n",
    "\n",
    "# Metropolis-Hastings acquisition\n",
    "def select_next_batch_metropolis(X_pool, mu, sigma, batch_size, steps=config['metropolis_steps']):\n",
    "    n = len(mu)\n",
    "    selected = set()\n",
    "    available_indices = set(range(n))\n",
    "\n",
    "    # Initialize with a random index\n",
    "    current_idx = np.random.choice(list(available_indices))\n",
    "    selected.add(current_idx)\n",
    "    available_indices.remove(current_idx)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        if len(selected) >= batch_size:\n",
    "            break\n",
    "        if not available_indices:\n",
    "            break  # pool exhausted\n",
    "\n",
    "        candidate_idx = np.random.choice(list(available_indices))\n",
    "\n",
    "        sample_current = np.random.normal(mu[current_idx], sigma[current_idx])\n",
    "        sample_candidate = np.random.normal(mu[candidate_idx], sigma[candidate_idx])\n",
    "\n",
    "        # Favor candidates with lower sample values\n",
    "        p_accept = min(1, (sample_current + 1e-6) / (sample_candidate + 1e-6))\n",
    "\n",
    "        if np.random.rand() < p_accept:\n",
    "            selected.add(candidate_idx)\n",
    "            available_indices.remove(candidate_idx)\n",
    "            current_idx = candidate_idx  # move to accepted\n",
    "\n",
    "    return np.array(list(selected))\n",
    "\n",
    "\n",
    "'''Define Learning Loop'''\n",
    "def gp_active_learning_loop(X, y, seed, nn_model, initial_ratio=0.01, batch_size=256, acquisition=config['acquisition']):\n",
    "    n_samples = X.shape[0]\n",
    "    initial_size = int(initial_ratio * n_samples)\n",
    "    all_indices = np.arange(n_samples)\n",
    "    np.random.shuffle(all_indices)\n",
    "    selected_indices = list(all_indices[:initial_size])\n",
    "    remaining_indices = all_indices[initial_size:]\n",
    "\n",
    "    total_batches = len(remaining_indices) // batch_size\n",
    "\n",
    "    with tqdm(total=total_batches, desc=f\"Active Learning ({acquisition})\") as pbar:\n",
    "    #for _ in range(total_batches):\n",
    "        while len(remaining_indices) >= batch_size:\n",
    "            subsample_size = min(config.get('subsample_pool_size', 10000), len(remaining_indices))\n",
    "            sub_pool_rel_indices = np.random.choice(len(remaining_indices), subsample_size, replace=False)\n",
    "            sub_pool_abs_indices = remaining_indices[sub_pool_rel_indices]\n",
    "\n",
    "            X_pool = X[sub_pool_abs_indices]\n",
    "            y_pool = y[sub_pool_abs_indices]\n",
    "            if len(remaining_indices) < batch_size:\n",
    "                break\n",
    "\n",
    "            X_train, y_train = X[selected_indices], y[selected_indices]\n",
    "            # define GP model here for acquisiton methods\n",
    "            if acquisition != 'passive':\n",
    "                if config['surrogate'] == 'gp':\n",
    "                    mu, sigma = fit_gpytorch_and_predict(X_train, y_train, X_pool, seed,\n",
    "                                                         training_iter=config['training_iter'],\n",
    "                                                         max_train_size=config['max_train_size'])\n",
    "\n",
    "                elif config['surrogate'] == 'rf':\n",
    "                    mu, sigma = fit_rf_sklearn_and_predict(X_train, y_train, X_pool, seed,\n",
    "                                                           n_estimators=config['n_estimators'],\n",
    "                                                           max_train_size=config['max_train_size'])\n",
    "\n",
    "                elif config['surrogate'] == 'bayes_linear':\n",
    "                    mu, sigma = fit_bayes_linear_and_predict(X_train, y_train, X_pool, seed,\n",
    "                                                             max_train_size=config['max_train_size'])\n",
    "\n",
    "                elif config['surrogate'] == 'mc_dropout':  # must use X_pool here\n",
    "                    mu, sigma = fit_mc_dropout_and_predict(nn_model, X_pool, T=config['mc_droupout_T'])\n",
    "\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown surrogate model: {config['surrogate']}\")\n",
    "\n",
    "\n",
    "            if acquisition == 'passive':\n",
    "                rel_indices = select_next_batch_passive(X_pool, batch_size)  # no need gp_model\n",
    "            elif acquisition == 'greedy':\n",
    "                rel_indices = select_next_batch_greedy(X_pool, y_pool, mu, sigma, batch_size)\n",
    "            elif acquisition == 'mcmc':\n",
    "                rel_indices = select_next_batch_mcmc(X_pool, mu, sigma, batch_size)\n",
    "            elif acquisition == 'thompson':\n",
    "                rel_indices = select_next_batch_thompson(X_pool, mu, sigma, batch_size)\n",
    "            elif acquisition == 'metropolis':\n",
    "                rel_indices = select_next_batch_metropolis(X_pool, mu, sigma, batch_size)\n",
    "                \n",
    "            else:\n",
    "                raise ValueError('unknown acquisiton type')\n",
    "            #print(len(rel_indices))\n",
    "            if len(rel_indices) < config['batch_size']:\n",
    "                print(f\"warning, undersampling detected: {len(rel_indices)} number sampled\")\n",
    "\n",
    "            #print(\"X_pool.shape:\", X_pool.shape)\n",
    "            #print(\"mu.shape:\", mu.shape)\n",
    "            #print(\"sigma.shape:\", sigma.shape)\n",
    "            #print(\"rel_indices.max():\", rel_indices.max())\n",
    "        \n",
    "            abs_indices = sub_pool_abs_indices[rel_indices]\n",
    "\n",
    "            selected_indices.extend(abs_indices)\n",
    "            remaining_indices = np.setdiff1d(remaining_indices, abs_indices)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    X_final = X[selected_indices]\n",
    "    y_final = y[selected_indices]\n",
    "    dataset = RegressionDataset(X_final, y_final)\n",
    "\n",
    "    # we do not want to shuffle in this case since we just made and ordered the batches based on acquisition\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1e78613-4113-4a48-89ad-8901afeda072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple sparse-aware MLP\n",
    "class SparseRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3cd2232-bf2f-4784-8605-3027e9a74f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106652, 70)\n"
     ]
    }
   ],
   "source": [
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert to tensors and normalize\n",
    "Xd_tr_tensor = torch.tensor(Xd_tr.to_numpy(), dtype=torch.float32)\n",
    "Xd_te_tensor = torch.tensor(Xd_te.to_numpy(), dtype=torch.float32)\n",
    "y_tr_tensor = torch.tensor(y_tr.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "y_te_tensor = torch.tensor(y_te.to_numpy(), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xd_tr_tensor = torch.tensor(scaler.fit_transform(Xd_tr_tensor), dtype=torch.float32)\n",
    "Xd_te_tensor = torch.tensor(scaler.transform(Xd_te_tensor), dtype=torch.float32)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TensorDataset(Xd_tr_tensor, y_tr_tensor)\n",
    "test_dataset = TensorDataset(Xd_te_tensor, y_te_tensor)\n",
    "\n",
    "# Split into train and val\n",
    "val_ratio = config['val_ratio']\n",
    "val_size = int(len(train_dataset) * val_ratio)\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Unpack tensors from train_dataset (already normalized + tensorized)\n",
    "train_X_tensor, train_y_tensor = zip(*train_dataset)\n",
    "\n",
    "# Stack and convert to NumPy arrays for GP input\n",
    "Xd_tr_np = torch.stack(train_X_tensor).cpu().numpy()\n",
    "y_tr_np = torch.stack(train_y_tensor).squeeze().cpu().numpy()\n",
    "\n",
    "print(Xd_tr_np.shape)\n",
    "\n",
    "#train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "# Do not shuffle, the randomness came from inside the loop.\n",
    "# Need to reload the model each epoch so the batch order is not part of the learning.\n",
    "'''train_loader = gp_active_learning_loop(Xd_tr_np, y_tr_np,  # ← Xd_tr and y_tr as NumPy arrays\n",
    "                                       config['random_seed'],\n",
    "                                       initial_ratio=config['initial_ratio'],\n",
    "                                       batch_size=config['batch_size'],\n",
    "                                       acquisition=config['acquisition'])'''\n",
    "\n",
    "val_loader   = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "test_loader  = DataLoader(test_dataset, batch_size=config['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92e71ff5-d126-4fde-b85b-bb62be91453e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. \\nDecreasing the bound and calling fit again may find a better value.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ConvergenceWarning: The optimal value found for dimension 0 of parameter length_scale is close to the specified lower bound 1e-10. \n",
    "Decreasing the bound and calling fit again may find a better value.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "396ddc0e-e201-4cc1-8a59-59d24284790f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 256]          18,176\n",
      "       BatchNorm1d-2                  [-1, 256]             512\n",
      "              GELU-3                  [-1, 256]               0\n",
      "           Dropout-4                  [-1, 256]               0\n",
      "            Linear-5                  [-1, 128]          32,896\n",
      "       BatchNorm1d-6                  [-1, 128]             256\n",
      "              GELU-7                  [-1, 128]               0\n",
      "           Dropout-8                  [-1, 128]               0\n",
      "            Linear-9                   [-1, 64]           8,256\n",
      "             GELU-10                   [-1, 64]               0\n",
      "           Linear-11                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 60,161\n",
      "Trainable params: 60,161\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.23\n",
      "Estimated Total Size (MB): 0.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Model setup\n",
    "input_dim = Xd_tr.shape[1]\n",
    "nn_model = SparseRegressor(input_dim, dropout_rate=config['dropout']).to(device)\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "summary(nn_model, input_size=(input_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a27fe2bd-bb20-4d08-9f16-09c42a4b10bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(pred, true):  # do not use this since loss is already MSE\n",
    "    return torch.sqrt(F.mse_loss(pred, true)).item()\n",
    "\n",
    "\n",
    "def compute_r2(pred, true):\n",
    "    ss_res = torch.sum((true - pred) ** 2)\n",
    "    ss_tot = torch.sum((true - torch.mean(true)) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2.item()\n",
    "    \n",
    "\n",
    "def train_model(nn_model, val_loader, test_loader, epochs=30):\n",
    "    \n",
    "    train_losses, val_losses, test_losses = [], [], []\n",
    "    train_r2s, val_r2s, test_r2s = [], [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        seed = config['random_seed'] + epoch  # different seed per epoch\n",
    "\n",
    "        train_loader = gp_active_learning_loop(\n",
    "            Xd_tr_np, y_tr_np,\n",
    "            seed,\n",
    "            nn_model,\n",
    "            initial_ratio=config['initial_ratio'],\n",
    "            batch_size=config['batch_size'],\n",
    "            acquisition=config['acquisition']\n",
    "        )\n",
    "        \n",
    "        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", leave=False)\n",
    "        for x_batch, y_batch in train_loop:\n",
    "            nn_model.train()\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = nn_model(x_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            r2 = compute_r2(preds, y_batch)\n",
    "            train_losses.append(loss.item())\n",
    "            train_r2s.append(r2)\n",
    "            train_loop.set_postfix(loss=loss.item(), r2=r2)\n",
    "\n",
    "            # Validation (per batch)\n",
    "            nn_model.eval()\n",
    "            #val_loop = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Validation\", leave=False)\n",
    "            total_val_loss = 0.0\n",
    "            total_val_r2 = 0.0\n",
    "            with torch.no_grad():\n",
    "                #for x_val, y_val in val_loop:\n",
    "                for x_val, y_val in val_loader:\n",
    "                    x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                    preds = nn_model(x_val)\n",
    "                    loss = criterion(preds, y_val)\n",
    "                    r2 = compute_r2(preds, y_val)\n",
    "                    total_val_loss += loss.item() * x_val.size(0)\n",
    "                    total_val_r2 += r2 * x_val.size(0)\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "            avg_val_r2 = total_val_r2 / len(val_loader.dataset)\n",
    "            val_losses.append(avg_val_loss)\n",
    "            val_r2s.append(avg_val_r2)\n",
    "            #val_loop.set_postfix(loss=loss.item(), r2=r2)\n",
    "\n",
    "            # Test (per batch)\n",
    "            #test_loop = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} - Test\", leave=False)\n",
    "            total_test_loss = 0.0\n",
    "            total_test_r2 = 0.0\n",
    "            with torch.no_grad():\n",
    "                #for x_test, y_test in test_loop:\n",
    "                for x_test, y_test in test_loader:\n",
    "                    x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "                    preds = nn_model(x_test)\n",
    "                    loss = criterion(preds, y_test)\n",
    "                    r2 = compute_r2(preds, y_test)\n",
    "                    total_test_loss += loss.item() * x_test.size(0)\n",
    "                    total_test_r2 += r2 * x_test.size(0)\n",
    "\n",
    "            avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "            avg_test_r2 = total_test_r2 / len(test_loader.dataset)\n",
    "            test_losses.append(avg_test_loss)\n",
    "            test_r2s.append(avg_test_r2)\n",
    "            #test_loop.set_postfix(loss=loss.item(), r2=r2)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d} | \"\n",
    "              f\"Last Batch -> Train Loss: {train_losses[-1]:.4f}, R^2: {train_r2s[-1]:.4f} | \"\n",
    "              f\"Val Loss: {val_losses[-1]:.4f}, R^2: {val_r2s[-1]:.4f} | \"\n",
    "              f\"Test Loss: {test_losses[-1]:.4f}, R^2: {test_r2s[-1]:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": train_losses, \"val_loss\": val_losses, \"test_loss\": test_losses,\n",
    "        \"train_r2\": train_r2s, \"val_r2\": val_r2s, \"test_r2\": test_r2s\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aff4dd-e20b-4270-a26d-15f0f73123c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Active Learning (metropolis): 100%|████████████████████████████████████████████████████| 51/51 [01:13<00:00,  1.45s/it]\n",
      "Epoch 1/10 - Training:  27%|█████████▉                           | 14/52 [00:23<00:58,  1.54s/it, loss=8.86, r2=-0.306]"
     ]
    }
   ],
   "source": [
    "results = train_model(nn_model, val_loader, test_loader, epochs=config['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c70741-2742-4310-b7c3-d338f5342622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each epoch has 52 batches for batch size 2048\n",
    "\n",
    "os.makedirs(f\"results_{config['random_seed']}\", exist_ok=True)\n",
    "\n",
    "np.savez(f\"results_{config['random_seed']}/history_{config['surrogate']}_{config['acquisition']}_rnd{config['random_seed']}.npz\", **results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92366cfd-d813-4ac7-ac29-7ce92fb2b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nn_model.state_dict(), f\"results_{config['random_seed']}/model_{config['surrogate']}_{config['acquisition']}_rnd{config['random_seed']}.pt\")\n",
    "\n",
    "'''model = SparseRegressor(input_dim).to(device)\n",
    "model.load_state_dict(torch.load(f\"checkpoints/model_{config['acquisition']}_seed{config['random_seed']}.pt\"))\n",
    "model.eval()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14980b51-d61c-410d-8fa8-6b837976d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = np.load(f\"results_{config['random_seed']}/history_{config['surrogate']}_{config['acquisition']}_rnd{config['random_seed']}.npz\")\n",
    "print(loaded[\"train_r2\"].shape)\n",
    "\n",
    "results = {k: list(loaded[k]) for k in loaded.files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89ef20-ecbf-4fde-9296-69a06d6ebb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = list(range(1, len(results[\"train_loss\"]) + 1))\n",
    "\n",
    "# Get min values and corresponding steps\n",
    "def get_min_metric(metric):\n",
    "    value = min(metric)\n",
    "    steps = metric.index(value) + 1\n",
    "    return value, steps\n",
    "\n",
    "\n",
    "# Get max values and corresponding steps\n",
    "def get_max_metric(metric):\n",
    "    value = max(metric)\n",
    "    steps = metric.index(value) + 1\n",
    "    return value, steps\n",
    "\n",
    "\n",
    "min_train_loss, ep_train_loss = get_min_metric(results[\"train_loss\"])\n",
    "min_val_loss, ep_val_loss     = get_min_metric(results[\"val_loss\"])\n",
    "min_test_loss, ep_test_loss   = get_min_metric(results[\"test_loss\"])\n",
    "\n",
    "max_train_r2, ep_train_r2 = get_max_metric(results[\"train_r2\"])\n",
    "max_val_r2, ep_val_r2     = get_max_metric(results[\"val_r2\"])\n",
    "max_test_r2, ep_test_r2   = get_max_metric(results[\"test_r2\"])\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# --- Loss Plot ---\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(steps, results[\"train_loss\"], label=\"Train\")\n",
    "plt.plot(steps, results[\"val_loss\"], label=\"Val\")\n",
    "plt.plot(steps, results[\"test_loss\"], label=\"Test\")\n",
    "\n",
    "# Mark min points for Loss\n",
    "plt.scatter(ep_train_loss, min_train_loss, color='green', label=f\"Min Train: {min_train_loss:.4f}\")\n",
    "plt.text(ep_train_loss, min_train_loss, f\"E{ep_train_loss}\", color='green', va='top', ha='right')\n",
    "\n",
    "plt.scatter(ep_val_loss, min_val_loss, color='red', label=f\"Min Val: {min_val_loss:.4f}\")\n",
    "plt.text(ep_val_loss, min_val_loss, f\"E{ep_val_loss}\", color='red', va='top', ha='right')\n",
    "\n",
    "plt.scatter(ep_test_loss, min_test_loss, color='blue', label=f\"Min Test: {min_test_loss:.4f}\")\n",
    "plt.text(ep_test_loss, min_test_loss, f\"E{ep_test_loss}\", color='blue', va='top', ha='right')\n",
    "\n",
    "plt.title(f\"{config['surrogate']} {config['acquisition']} rnd {config['random_seed']} Loss (MSE)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# --- R^2 Plot ---\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(steps, results[\"train_r2\"], label=\"Train\")\n",
    "plt.plot(steps, results[\"val_r2\"], label=\"Val\")\n",
    "plt.plot(steps, results[\"test_r2\"], label=\"Test\")\n",
    "\n",
    "# Mark max points for R^2\n",
    "plt.scatter(ep_train_r2, max_train_r2, color='green', label=f\"Max Train: {max_train_r2:.4f}\")\n",
    "plt.text(ep_train_r2, max_train_r2, f\"E{ep_train_r2}\", color='green', va='top', ha='right')\n",
    "\n",
    "plt.scatter(ep_val_r2, max_val_r2, color='red', label=f\"Max Val: {max_val_r2:.4f}\")\n",
    "plt.text(ep_val_r2, max_val_r2, f\"E{ep_val_r2}\", color='red', va='top', ha='right')\n",
    "\n",
    "plt.scatter(ep_test_r2, max_test_r2, color='blue', label=f\"Max Test: {max_test_r2:.4f}\")\n",
    "plt.text(ep_test_r2, max_test_r2, f\"E{ep_test_r2}\", color='blue', va='top', ha='right')\n",
    "\n",
    "plt.title(f\"{config['surrogate']} {config['acquisition']} rnd {config['random_seed']} R^2\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"R^2\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681cb834-491d-43f1-a4d8-97daa9b5f2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_env_p11)",
   "language": "python",
   "name": "pytorch_env_p11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
